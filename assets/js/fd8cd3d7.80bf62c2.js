"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[227],{3905:(e,r,n)=>{n.d(r,{Zo:()=>d,kt:()=>m});var t=n(7294);function o(e,r,n){return r in e?Object.defineProperty(e,r,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[r]=n,e}function i(e,r){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);r&&(t=t.filter((function(r){return Object.getOwnPropertyDescriptor(e,r).enumerable}))),n.push.apply(n,t)}return n}function s(e){for(var r=1;r<arguments.length;r++){var n=null!=arguments[r]?arguments[r]:{};r%2?i(Object(n),!0).forEach((function(r){o(e,r,n[r])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(r){Object.defineProperty(e,r,Object.getOwnPropertyDescriptor(n,r))}))}return e}function a(e,r){if(null==e)return{};var n,t,o=function(e,r){if(null==e)return{};var n,t,o={},i=Object.keys(e);for(t=0;t<i.length;t++)n=i[t],r.indexOf(n)>=0||(o[n]=e[n]);return o}(e,r);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(t=0;t<i.length;t++)n=i[t],r.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var l=t.createContext({}),c=function(e){var r=t.useContext(l),n=r;return e&&(n="function"==typeof e?e(r):s(s({},r),e)),n},d=function(e){var r=c(e.components);return t.createElement(l.Provider,{value:r},e.children)},_="mdxType",p={inlineCode:"code",wrapper:function(e){var r=e.children;return t.createElement(t.Fragment,{},r)}},v=t.forwardRef((function(e,r){var n=e.components,o=e.mdxType,i=e.originalType,l=e.parentName,d=a(e,["components","mdxType","originalType","parentName"]),_=c(n),v=o,m=_["".concat(l,".").concat(v)]||_[v]||p[v]||i;return n?t.createElement(m,s(s({ref:r},d),{},{components:n})):t.createElement(m,s({ref:r},d))}));function m(e,r){var n=arguments,o=r&&r.mdxType;if("string"==typeof e||o){var i=n.length,s=new Array(i);s[0]=v;var a={};for(var l in r)hasOwnProperty.call(r,l)&&(a[l]=r[l]);a.originalType=e,a[_]="string"==typeof e?e:o,s[1]=a;for(var c=2;c<i;c++)s[c]=n[c];return t.createElement.apply(null,s)}return t.createElement.apply(null,n)}v.displayName="MDXCreateElement"},3358:(e,r,n)=>{n.r(r),n.d(r,{assets:()=>l,contentTitle:()=>s,default:()=>_,frontMatter:()=>i,metadata:()=>a,toc:()=>c});var t=n(7462),o=(n(7294),n(3905));const i={sidebar_position:7},s="Word2Vec",a={unversionedId:"w_to_vec",id:"w_to_vec",title:"Word2Vec",description:"7 Word2Vec",source:"@site/docs/w_to_vec.md",sourceDirName:".",slug:"/w_to_vec",permalink:"/nlp_project/docs/w_to_vec",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/w_to_vec.md",tags:[],version:"current",sidebarPosition:7,frontMatter:{sidebar_position:7},sidebar:"tutorialSidebar",previous:{title:"SVM",permalink:"/nlp_project/docs/svm"},next:{title:"LDA",permalink:"/nlp_project/docs/lda"}},l={},c=[{value:"7 Word2Vec",id:"7-word2vec",level:3},{value:"7.2 Word2Vec Google model",id:"72-word2vec-google-model",level:3}],d={toc:c};function _(e){let{components:r,...n}=e;return(0,o.kt)("wrapper",(0,t.Z)({},d,n,{components:r,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"word2vec"},"Word2Vec"),(0,o.kt)("h3",{id:"7-word2vec"},"7 Word2Vec"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"from nltk import sent_tokenize\nimport gensim\nfrom sklearn.model_selection import train_test_split\nfrom gensim.utils import simple_preprocess\n\ncorpus = [sent_tokenize(review) for review in df.cleaned_news.values.tolist()]\n\nwords = []\nlabels = []\nfor rev,label in zip(df.cleaned_news.values.tolist(), df.category.values.tolist()):\n    sent_rev = sent_tokenize(rev)\n    for word in sent_rev:\n        words.append(simple_preprocess(word))\n        labels.append(label)\n\n\nmodel_w2v = gensim.models.Word2Vec(words, window = 5, min_count = 1)\n\ndef review_embeddings(review,model_w2v, method:str = 'avg'):\n    \"\"\"\n    Return the Text vector using the average or sum of word embeddings given by Word2Vec\n    \"\"\"\n    if method == 'avg':\n        return np.mean([model_w2v.wv[word] for word in review if word in model_w2v.wv.index_to_key], axis = 0)\n    return np.sum([model_w2v.wv[word] for word in review if word in model_w2v.wv.index_to_key], axis = 0)\n\nX_reviews = [review_embeddings(review,model_w2v) for review in words]\nX_reviews = np.array(X_reviews)\nlabel_dict  ={'business':0, 'entertainment':1,'politics':2,'sport':3,'tech':4}\ny_reviews = [label_dict[label] for label in labels]\nX_train, X_test, y_train, y_test = train_test_split(X_reviews, y_reviews, test_size =0.2)\n\nfrom sklearn.linear_model import LogisticRegression\nlr_w2v = LogisticRegression()\n\nX_train_corr = []\ny_train_corr = []\nfor i,y in zip(X_train,y_train):\n    try:\n        if i.shape[0] == 100:\n            X_train_corr.append(i)\n            y_train_corr.append(y)\n    except:\n        print(i)\n\nX_train_corr = np.array(X_train_corr)\nX_test_corr = np.array(X_test_corr)\n\n\nlr_w2v.fit(X_train_corr, y_train_corr)\n\nfrom sklearn.metrics import accuracy_score\n\naccuracy_score(y_test_corr,lr_w2v.predict(X_test_corr))#0.9191011235955057\n\nmodel_w2v.wv.most_similar('business')\n\"\"\"\n[('buy', 0.9954310655593872),\n ('operating', 0.9928811192512512),\n ('reduce', 0.992861270904541),\n ('offer', 0.9926395416259766),\n ('credit', 0.9919050931930542),\n ('regulator', 0.9917522072792053),\n ('carrier', 0.9914599061012268),\n ('offshore', 0.991354763507843),\n ('seeking', 0.9912638664245605),\n ('energy', 0.9910051226615906)]\n \"\"\"\n # save model\nimport pickle\n\n# save the model to disk\nfilename = 'w2v_lr_model.sav'\npickle.dump(lr_w2v, open(filename, 'wb'))\n\n")),(0,o.kt)("h3",{id:"72-word2vec-google-model"},"7.2 Word2Vec Google model"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"import gensim.downloader\nmodel = gensim.downloader.load(\"word2vec-google-news-300\")\n\nmodel\n\ndef review_embeddings_2(review,model_w2v, method:str = 'avg'):\n    \"\"\"\n    Return the Text vector using the average or sum of word embeddings given by Word2Vec\n    \"\"\"\n    if method == 'avg':\n        return np.mean([model_w2v[word] for word in review if word in model_w2v], axis = 0)\n    return np.sum([model_w2v.wv[word] for word in review if word in model_w2v], axis = 0)\n\nX_reviews = [review_embeddings_2(review,model) for review in words]\nX_reviews = np.array(X_reviews)\nlabel_dict  ={'business':0, 'entertainment':1,'politics':2,'sport':3,'tech':4}\ny_reviews = [label_dict[label] for label in labels]\nX_train, X_test, y_train, y_test = train_test_split(X_reviews, y_reviews, test_size =0.2)\n\nlr_w2v_2 = LogisticRegression()\n\nlr_w2v_2.fit(X_train, y_train)\n\naccuracy_score(y_test,lr_w2v_2.predict(X_test))#0.950561797752809\n")))}_.isMDXComponent=!0}}]);