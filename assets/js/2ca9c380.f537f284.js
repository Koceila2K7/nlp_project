"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[661],{3905:(e,t,r)=>{r.d(t,{Zo:()=>c,kt:()=>f});var n=r(7294);function a(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function o(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),r.push.apply(r,n)}return r}function s(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?o(Object(r),!0).forEach((function(t){a(e,t,r[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):o(Object(r)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))}))}return e}function i(e,t){if(null==e)return{};var r,n,a=function(e,t){if(null==e)return{};var r,n,a={},o=Object.keys(e);for(n=0;n<o.length;n++)r=o[n],t.indexOf(r)>=0||(a[r]=e[r]);return a}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)r=o[n],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(a[r]=e[r])}return a}var l=n.createContext({}),u=function(e){var t=n.useContext(l),r=t;return e&&(r="function"==typeof e?e(t):s(s({},t),e)),r},c=function(e){var t=u(e.components);return n.createElement(l.Provider,{value:t},e.children)},d="mdxType",p={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},m=n.forwardRef((function(e,t){var r=e.components,a=e.mdxType,o=e.originalType,l=e.parentName,c=i(e,["components","mdxType","originalType","parentName"]),d=u(r),m=a,f=d["".concat(l,".").concat(m)]||d[m]||p[m]||o;return r?n.createElement(f,s(s({ref:t},c),{},{components:r})):n.createElement(f,s({ref:t},c))}));function f(e,t){var r=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var o=r.length,s=new Array(o);s[0]=m;var i={};for(var l in t)hasOwnProperty.call(t,l)&&(i[l]=t[l]);i.originalType=e,i[d]="string"==typeof e?e:a,s[1]=i;for(var u=2;u<o;u++)s[u]=r[u];return n.createElement.apply(null,s)}return n.createElement.apply(null,r)}m.displayName="MDXCreateElement"},6420:(e,t,r)=>{r.r(t),r.d(t,{assets:()=>l,contentTitle:()=>s,default:()=>d,frontMatter:()=>o,metadata:()=>i,toc:()=>u});var n=r(7462),a=(r(7294),r(3905));const o={sidebar_position:9},s="BERT",i={unversionedId:"bert",id:"bert",title:"BERT",description:"BERT (Bidirectional Encoder Representations from Transformers) est un mod\xe8le de traitement du langage naturel d\xe9velopp\xe9 par Google en 2018. C'est un mod\xe8le de pr\xe9traitement de texte qui peut \xeatre utilis\xe9 comme base pour diff\xe9rentes t\xe2ches de traitement du langage naturel, telles que la reconnaissance de la parole, la traduction automatique et la compr\xe9hension du langage naturel.",source:"@site/docs/bert.md",sourceDirName:".",slug:"/bert",permalink:"/nlp_project/docs/bert",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/bert.md",tags:[],version:"current",sidebarPosition:9,frontMatter:{sidebar_position:9},sidebar:"tutorialSidebar",previous:{title:"LDA",permalink:"/nlp_project/docs/lda"}},l={},u=[{value:"9 BERT",id:"9-bert",level:3}],c={toc:u};function d(e){let{components:t,...r}=e;return(0,a.kt)("wrapper",(0,n.Z)({},c,r,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"bert"},"BERT"),(0,a.kt)("p",null,"BERT (Bidirectional Encoder Representations from Transformers) est un mod\xe8le de traitement du langage naturel d\xe9velopp\xe9 par Google en 2018. C'est un mod\xe8le de pr\xe9traitement de texte qui peut \xeatre utilis\xe9 comme base pour diff\xe9rentes t\xe2ches de traitement du langage naturel, telles que la reconnaissance de la parole, la traduction automatique et la compr\xe9hension du langage naturel."),(0,a.kt)("p",null,"BERT utilise un r\xe9seau de neurones bas\xe9 sur le mod\xe8le de transformer, qui est un mod\xe8le de traitement du langage naturel utilisant des attentions pour traiter des s\xe9quences de mots dans un ordre arbitraire. BERT est bidirectionnel, ce qui signifie qu'il prend en compte le contexte des mots \xe0 la fois avant et apr\xe8s chaque mot dans une phrase, ce qui lui permet d'avoir une meilleure compr\xe9hension du sens des mots dans le contexte de la phrase."),(0,a.kt)("p",null,"BERT a \xe9t\xe9 entra\xeen\xe9 sur de grandes collections de textes et a atteint de tr\xe8s bons r\xe9sultats dans de nombreuses t\xe2ches de traitement du langage naturel. Il est souvent utilis\xe9 comme mod\xe8le de base pour de nombreuses t\xe2ches de traitement du langage naturel et a \xe9t\xe9 largement adopt\xe9 par de nombreuses entreprises et organisations du monde entier."),(0,a.kt)("h3",{id:"9-bert"},"9 BERT"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"from transformers import BertTokenizer\nbert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nfrom transformers import BertForSequenceClassification\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased',\n                                                      num_labels = 5,  problem_type=\"multi_label_classification\",labels=['test1','test2','test3','test4','test5'])\n\ntext = 'economy'\ninputs = bert_tokenizer(text, return_tensors=\"pt\")\nimport torch\n\nwith torch.no_grad():\n\n    logits = model(**inputs).logits\n\nprint(model.config.id2label)\n#{0: 'LABEL_0', 1: 'LABEL_1', 2: 'LABEL_2', 3: 'LABEL_3', 4: 'LABEL_4'}\npredicted_class_id = logits.argmax().item()\n\nmodel.config.id2label[predicted_class_id]\n\n")))}d.isMDXComponent=!0}}]);