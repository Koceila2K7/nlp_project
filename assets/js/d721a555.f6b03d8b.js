"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[662],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>f});var r=n(7294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function s(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function a(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?s(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):s(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function i(e,t){if(null==e)return{};var n,r,o=function(e,t){if(null==e)return{};var n,r,o={},s=Object.keys(e);for(r=0;r<s.length;r++)n=s[r],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(r=0;r<s.length;r++)n=s[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var u=r.createContext({}),l=function(e){var t=r.useContext(u),n=t;return e&&(n="function"==typeof e?e(t):a(a({},t),e)),n},c=function(e){var t=l(e.components);return r.createElement(u.Provider,{value:t},e.children)},d="mdxType",p={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},m=r.forwardRef((function(e,t){var n=e.components,o=e.mdxType,s=e.originalType,u=e.parentName,c=i(e,["components","mdxType","originalType","parentName"]),d=l(n),m=o,f=d["".concat(u,".").concat(m)]||d[m]||p[m]||s;return n?r.createElement(f,a(a({ref:t},c),{},{components:n})):r.createElement(f,a({ref:t},c))}));function f(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var s=n.length,a=new Array(s);a[0]=m;var i={};for(var u in t)hasOwnProperty.call(t,u)&&(i[u]=t[u]);i.originalType=e,i[d]="string"==typeof e?e:o,a[1]=i;for(var l=2;l<s;l++)a[l]=n[l];return r.createElement.apply(null,a)}return r.createElement.apply(null,n)}m.displayName="MDXCreateElement"},573:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>u,contentTitle:()=>a,default:()=>d,frontMatter:()=>s,metadata:()=>i,toc:()=>l});var r=n(7462),o=(n(7294),n(3905));const s={sidebar_position:3},a="Bag of word and TF IDF",i={unversionedId:"bagofword",id:"bagofword",title:"Bag of word and TF IDF",description:"Bag of word",source:"@site/docs/bagofword.md",sourceDirName:".",slug:"/bagofword",permalink:"/nlp_project/docs/bagofword",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/bagofword.md",tags:[],version:"current",sidebarPosition:3,frontMatter:{sidebar_position:3},sidebar:"tutorialSidebar",previous:{title:"Nettoyage des donn\xe9es",permalink:"/nlp_project/docs/nettoyage"},next:{title:"Logistic Regression",permalink:"/nlp_project/docs/logestic_reg"}},u={},l=[{value:"Bag of word",id:"bag-of-word",level:3},{value:"TFIDF",id:"tfidf",level:3},{value:"3.1 Construire les vecteurs BoW &amp; Tfidf",id:"31-construire-les-vecteurs-bow--tfidf",level:3},{value:"CountVectorizer &amp; TfidfVectorizer",id:"countvectorizer--tfidfvectorizer",level:4},{value:"3.2 Split Train / Test",id:"32-split-train--test",level:2}],c={toc:l};function d(e){let{components:t,...n}=e;return(0,o.kt)("wrapper",(0,r.Z)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"bag-of-word-and-tf-idf"},"Bag of word and TF IDF"),(0,o.kt)("h3",{id:"bag-of-word"},"Bag of word"),(0,o.kt)("p",null,"La technique Bag of Words (sac de mots en fran\xe7ais) est un mod\xe8le de traitement de textes qui consiste \xe0 repr\xe9senter un texte sous la forme d'un sac de mots (ou vecteur de mots). Cette technique consiste \xe0 ignorer la grammaire et l'ordre des mots dans le texte et \xe0 se concentrer uniquement sur la pr\xe9sence ou l'absence de chaque mot dans le texte."),(0,o.kt)("p",null,"Pour repr\xe9senter un texte sous la forme d'un sac de mots, on commence par cr\xe9er un vocabulaire de tous les mots possibles qui peuvent appara\xeetre dans les textes. Ensuite, pour chaque texte, on compte le nombre d'occurrences de chaque mot dans le vocabulaire et on cr\xe9e un vecteur de comptage qui indique combien de fois chaque mot du vocabulaire appara\xeet dans le texte."),(0,o.kt)("p",null,"La technique Bag of Words est souvent utilis\xe9e en traitement de langage naturel et en apprentissage automatique pour analyser et classer des textes. Elle est particuli\xe8rement utile pour traiter des textes de grande taille ou de grandes quantit\xe9s de textes car elle permet de repr\xe9senter efficacement le contenu d'un texte sans tenir compte de sa structure syntaxique ou de son ordre de mots."),(0,o.kt)("h3",{id:"tfidf"},"TFIDF"),(0,o.kt)("p",null,"TF-IDF (abr\xe9viation de term frequency-inverse document frequency, fr\xe9quence des termes - inverse de la fr\xe9quence des documents en fran\xe7ais) est une technique de traitement de textes utilis\xe9e pour \xe9valuer l'importance d'un mot dans un document par rapport \xe0 un ensemble de documents."),(0,o.kt)("p",null,"La fr\xe9quence des termes (TF) mesure la fr\xe9quence d'apparition d'un mot dans un document. Plus un mot appara\xeet souvent dans un document, plus sa fr\xe9quence est \xe9lev\xe9e."),(0,o.kt)("p",null,"L'inverse de la fr\xe9quence des documents (IDF) mesure l'importance d'un mot dans l'ensemble des documents. Plus un mot est rare dans l'ensemble des documents, plus son IDF est \xe9lev\xe9."),(0,o.kt)("p",null,"Pour calculer le score TF-IDF d'un mot dans un document, on multiplie sa fr\xe9quence par son IDF. Ainsi, les mots qui sont fr\xe9quents dans un document mais rares dans l'ensemble des documents ont un score TF-IDF \xe9lev\xe9, indiquant qu'ils sont importants pour le document en question."),(0,o.kt)("p",null,"TF-IDF est souvent utilis\xe9e en traitement de langage naturel et en apprentissage automatique pour analyser et classer des documents. Elle peut \xeatre utilis\xe9e pour extraire des mots-cl\xe9s d'un document, pour trouver des documents similaires ou pour classer des documents en fonction de leur contenu."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"from sklearn.model_selection import train_test_split\nX = df['cleaned_news'].values.tolist()\ny = df['category'].values.tolist()\n")),(0,o.kt)("h3",{id:"31-construire-les-vecteurs-bow--tfidf"},"3.1 Construire les vecteurs BoW & Tfidf"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n")),(0,o.kt)("h4",{id:"countvectorizer--tfidfvectorizer"},"CountVectorizer & TfidfVectorizer"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"cv = CountVectorizer(stop_words='english')\ncv_X = cv.fit_transform(X)\nprint(cv_X.shape)\n\ntf = TfidfVectorizer(stop_words='english') #ici on peut ajouter min_df, max_df. voir la doc de TfidfVectorizer !!\ntf_X = tf.fit_transform(X)\nprint(tf_X.shape)\n")),(0,o.kt)("h2",{id:"32-split-train--test"},"3.2 Split Train / Test"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(len(X_train))\n\ncv_train = cv.transform(X_train)\ncv_test = cv.transform(X_test)\n\nfrom sklearn import preprocessing\n\nle = preprocessing.LabelEncoder()\n\nle.fit(df.category.unique())\n\nle.classes_\n\ntrain_category = le.fit_transform(y_train)\ntest_category = le.fit_transform(y_test)\n\n#classes labels\n#le.inverse_transform([0, 1, 2, 3, 4])\nd = zip([0, 1, 2, 3, 4], le.inverse_transform([0, 1, 2, 3, 4]))\nclass_labels_dict = {}\nfor k,v in d:\n    class_labels_dict[k] = v\n\nclass_labels_dict\n\n#{0: 'business', 1: 'entertainment', 2: 'politics', 3: 'sport', 4: 'tech'}\n")))}d.isMDXComponent=!0}}]);