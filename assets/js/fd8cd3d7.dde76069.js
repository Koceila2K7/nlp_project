"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[227],{3905:(e,r,n)=>{n.d(r,{Zo:()=>c,kt:()=>v});var t=n(7294);function o(e,r,n){return r in e?Object.defineProperty(e,r,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[r]=n,e}function i(e,r){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);r&&(t=t.filter((function(r){return Object.getOwnPropertyDescriptor(e,r).enumerable}))),n.push.apply(n,t)}return n}function s(e){for(var r=1;r<arguments.length;r++){var n=null!=arguments[r]?arguments[r]:{};r%2?i(Object(n),!0).forEach((function(r){o(e,r,n[r])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(r){Object.defineProperty(e,r,Object.getOwnPropertyDescriptor(n,r))}))}return e}function a(e,r){if(null==e)return{};var n,t,o=function(e,r){if(null==e)return{};var n,t,o={},i=Object.keys(e);for(t=0;t<i.length;t++)n=i[t],r.indexOf(n)>=0||(o[n]=e[n]);return o}(e,r);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(t=0;t<i.length;t++)n=i[t],r.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var l=t.createContext({}),d=function(e){var r=t.useContext(l),n=r;return e&&(n="function"==typeof e?e(r):s(s({},r),e)),n},c=function(e){var r=d(e.components);return t.createElement(l.Provider,{value:r},e.children)},p="mdxType",u={inlineCode:"code",wrapper:function(e){var r=e.children;return t.createElement(t.Fragment,{},r)}},m=t.forwardRef((function(e,r){var n=e.components,o=e.mdxType,i=e.originalType,l=e.parentName,c=a(e,["components","mdxType","originalType","parentName"]),p=d(n),m=o,v=p["".concat(l,".").concat(m)]||p[m]||u[m]||i;return n?t.createElement(v,s(s({ref:r},c),{},{components:n})):t.createElement(v,s({ref:r},c))}));function v(e,r){var n=arguments,o=r&&r.mdxType;if("string"==typeof e||o){var i=n.length,s=new Array(i);s[0]=m;var a={};for(var l in r)hasOwnProperty.call(r,l)&&(a[l]=r[l]);a.originalType=e,a[p]="string"==typeof e?e:o,s[1]=a;for(var d=2;d<i;d++)s[d]=n[d];return t.createElement.apply(null,s)}return t.createElement.apply(null,n)}m.displayName="MDXCreateElement"},3358:(e,r,n)=>{n.r(r),n.d(r,{assets:()=>l,contentTitle:()=>s,default:()=>p,frontMatter:()=>i,metadata:()=>a,toc:()=>d});var t=n(7462),o=(n(7294),n(3905));const i={sidebar_position:7},s="Word2Vec",a={unversionedId:"w_to_vec",id:"w_to_vec",title:"Word2Vec",description:"7 Word2Vec",source:"@site/docs/w_to_vec.md",sourceDirName:".",slug:"/w_to_vec",permalink:"/nlp_project/docs/w_to_vec",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/w_to_vec.md",tags:[],version:"current",sidebarPosition:7,frontMatter:{sidebar_position:7},sidebar:"tutorialSidebar",previous:{title:"SVM",permalink:"/nlp_project/docs/svm"},next:{title:"LDA",permalink:"/nlp_project/docs/lda"}},l={},d=[{value:"7 Word2Vec",id:"7-word2vec",level:3},{value:"7.2 Word2Vec Google model",id:"72-word2vec-google-model",level:3}],c={toc:d};function p(e){let{components:r,...n}=e;return(0,o.kt)("wrapper",(0,t.Z)({},c,n,{components:r,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"word2vec"},"Word2Vec"),(0,o.kt)("h3",{id:"7-word2vec"},"7 Word2Vec"),(0,o.kt)("p",null,'Word2vec est un mod\xe8le de traitement de langage naturel utilis\xe9 pour apprendre des repr\xe9sentations vectorielles de mots (appel\xe9es "vecteurs de mots") \xe0 partir de grandes collections de textes. Ces vecteurs de mots sont des repr\xe9sentations num\xe9riques de chaque mot qui refl\xe8tent leur signification ou leur sens dans le contexte des autres mots.'),(0,o.kt)("p",null,"Word2vec utilise un r\xe9seau de neurones pour apprendre ces vecteurs de mots. Le mod\xe8le prend en entr\xe9e un texte et pr\xe9dit le mot suivant dans le texte en utilisant le mot courant et les mots pr\xe9c\xe9dents comme entr\xe9es. Le mod\xe8le est entra\xeen\xe9 sur une grande collection de textes de mani\xe8re \xe0 minimiser l'erreur de pr\xe9diction du mot suivant."),(0,o.kt)("p",null,"Une fois entra\xeen\xe9, le mod\xe8le peut \xeatre utilis\xe9 pour obtenir des vecteurs de mots pour n'importe quel mot dans le vocabulaire. Ces vecteurs de mots peuvent \xeatre utilis\xe9s dans de nombreuses t\xe2ches de traitement de langage naturel, telles que la reconnaissance de la parole, la traduction automatique et la recherche de documents similaires."),(0,o.kt)("p",null,"Word2vec a \xe9t\xe9 largement utilis\xe9 dans de nombreux domaines et a \xe9t\xe9 tr\xe8s influent dans le d\xe9veloppement de l'apprentissage automatique pour le traitement du langage naturel."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"from nltk import sent_tokenize\nimport gensim\nfrom sklearn.model_selection import train_test_split\nfrom gensim.utils import simple_preprocess\n\ncorpus = [sent_tokenize(review) for review in df.cleaned_news.values.tolist()]\n\nwords = []\nlabels = []\nfor rev,label in zip(df.cleaned_news.values.tolist(), df.category.values.tolist()):\n    sent_rev = sent_tokenize(rev)\n    for word in sent_rev:\n        words.append(simple_preprocess(word))\n        labels.append(label)\n\n\nmodel_w2v = gensim.models.Word2Vec(words, window = 5, min_count = 1)\n\ndef review_embeddings(review,model_w2v, method:str = 'avg'):\n    \"\"\"\n    Return the Text vector using the average or sum of word embeddings given by Word2Vec\n    \"\"\"\n    if method == 'avg':\n        return np.mean([model_w2v.wv[word] for word in review if word in model_w2v.wv.index_to_key], axis = 0)\n    return np.sum([model_w2v.wv[word] for word in review if word in model_w2v.wv.index_to_key], axis = 0)\n\nX_reviews = [review_embeddings(review,model_w2v) for review in words]\nX_reviews = np.array(X_reviews)\nlabel_dict  ={'business':0, 'entertainment':1,'politics':2,'sport':3,'tech':4}\ny_reviews = [label_dict[label] for label in labels]\nX_train, X_test, y_train, y_test = train_test_split(X_reviews, y_reviews, test_size =0.2)\n\nfrom sklearn.linear_model import LogisticRegression\nlr_w2v = LogisticRegression()\n\nX_train_corr = []\ny_train_corr = []\nfor i,y in zip(X_train,y_train):\n    try:\n        if i.shape[0] == 100:\n            X_train_corr.append(i)\n            y_train_corr.append(y)\n    except:\n        print(i)\n\nX_train_corr = np.array(X_train_corr)\nX_test_corr = np.array(X_test_corr)\n\n\nlr_w2v.fit(X_train_corr, y_train_corr)\n\nfrom sklearn.metrics import accuracy_score\n\naccuracy_score(y_test_corr,lr_w2v.predict(X_test_corr))#0.9191011235955057\n\nmodel_w2v.wv.most_similar('business')\n\"\"\"\n[('buy', 0.9954310655593872),\n ('operating', 0.9928811192512512),\n ('reduce', 0.992861270904541),\n ('offer', 0.9926395416259766),\n ('credit', 0.9919050931930542),\n ('regulator', 0.9917522072792053),\n ('carrier', 0.9914599061012268),\n ('offshore', 0.991354763507843),\n ('seeking', 0.9912638664245605),\n ('energy', 0.9910051226615906)]\n \"\"\"\n # save model\nimport pickle\n\n# save the model to disk\nfilename = 'w2v_lr_model.sav'\npickle.dump(lr_w2v, open(filename, 'wb'))\n\n")),(0,o.kt)("h3",{id:"72-word2vec-google-model"},"7.2 Word2Vec Google model"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"import gensim.downloader\nmodel = gensim.downloader.load(\"word2vec-google-news-300\")\n\nmodel\n\ndef review_embeddings_2(review,model_w2v, method:str = 'avg'):\n    \"\"\"\n    Return the Text vector using the average or sum of word embeddings given by Word2Vec\n    \"\"\"\n    if method == 'avg':\n        return np.mean([model_w2v[word] for word in review if word in model_w2v], axis = 0)\n    return np.sum([model_w2v.wv[word] for word in review if word in model_w2v], axis = 0)\n\nX_reviews = [review_embeddings_2(review,model) for review in words]\nX_reviews = np.array(X_reviews)\nlabel_dict  ={'business':0, 'entertainment':1,'politics':2,'sport':3,'tech':4}\ny_reviews = [label_dict[label] for label in labels]\nX_train, X_test, y_train, y_test = train_test_split(X_reviews, y_reviews, test_size =0.2)\n\nlr_w2v_2 = LogisticRegression()\n\nlr_w2v_2.fit(X_train, y_train)\n\naccuracy_score(y_test,lr_w2v_2.predict(X_test))#0.950561797752809\n")))}p.isMDXComponent=!0}}]);